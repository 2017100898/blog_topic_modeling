# ë„¤ì´ë²„ ë¸”ë¡œê·¸ í¬ìŠ¤íŒ… í† í”½ ëª¨ë¸ë§
2020-1 ê²½í¬ëŒ€í•™êµ ë°ì´í„°ë¶„ì„ ìº¡ìŠ¤í†¤ë””ìì¸ ì‚°ì—…ê²½ì˜ê³µí•™ê³¼ ë°•í˜œì›<br>
(ì €ì‘ê¶Œ ë¬¸ì œë¡œ ê´€ë ¨ ë°ì´í„° ì‚­ì œ í›„ git ê³µê°œí•©ë‹ˆë‹¤)   <br><br>
[![name](https://img.shields.io/static/v1?&label=visualization&message=LDA&color=pink)](http://htmlpreview.github.io/?https://github.com/2017100898/BlogTopic/blob/master/vis/LDA.html) [![name](https://img.shields.io/static/v1?&label=visualization&message=Kmeans&color=lavender)](http://htmlpreview.github.io/?https://github.com/2017100898/BlogTopic/blob/master/vis/kmeans.html)  ğŸ‘ˆğŸ‘ˆğŸ‘ˆ ğŸ’ğŸ’«ğŸš€âœ¨

## ë°°ê²½ ë° í•„ìš”ì„±
 * ë„¤ì´ë²„ ë¸”ë¡œê·¸ì—ì„œëŠ” ë¸”ë¡œê·¸ ì£¼ì œì™€ ê° í¬ìŠ¤íŒ…ì˜ ì£¼ì œë¥¼ ì„¤ì •í•  ìˆ˜ ìˆìœ¼ë‚˜, ì´ëŠ” ë¸”ë¡œê±°ê°€ ì§ì ‘ ì„ íƒí•˜ëŠ” ê²ƒì´ê¸° ë•Œë¬¸ì— **ì‹¤ì œ ì£¼ì œì™€ ëª…ëª©ìƒ ì£¼ì œê°€ ë‹¤ë¥¸ ê²½ìš°ê°€ ë§ë‹¤.** 
 
 * ë˜í•œ, ì—¬ëŸ¬ ì£¼ì œê°€ ì„ì—¬ ìˆëŠ” ê²½ìš°ëŠ” ë¸”ë¡œê±° ìì‹ ë„ ë¸”ë¡œê·¸ì˜ ì£¼ì œì™€ ë°©í–¥ì„±ì„ ì•Œì§€ ëª»í•  ë•Œê°€ ìˆë‹¤.  

 * ë„¤ì´ë²„ ë¸”ë¡œê·¸ëŠ” í•­ìƒ ```ì „ë¬¸ì„±``` ì´ ìˆì„ìˆ˜ë¡ ì¢‹ë‹¤. ìµœê·¼ ëª¨ì§‘ ì¤‘ì¸ ë„¤ì´ë²„ ì¸í”Œë£¨ì–¸ì„œ(ë˜ëŠ” ì´ì „ì˜ íŒŒì›Œë¸”ë¡œê±°)ëŠ” ì „ë¬¸ì„±ì´ ìˆì–´ì•¼ë§Œ ì§€ì›í•  ìˆ˜ ìˆìœ¼ë©°, **í¬ìŠ¤íŒ… í’ˆì§ˆ ê²°ì • ë° ë…¸ì¶œ ìˆœìœ„** ë“± ì—¬ëŸ¬ ë¶€ë¶„ì—ì„œ ë¸”ë¡œê±° ì „ë¬¸ì„±ì´ ì˜í–¥ì„ ë¯¸ì¹˜ê¸° ë•Œë¬¸ì´ë‹¤.  

 * ê´‘ê³ ì£¼ ê°™ì€ ê²½ìš°ì—ë„ ë¸”ë¡œê·¸ì˜ ì£¼ì œê°€ ì¼ê´€ëœ ì „ë¬¸ ë¸”ë¡œê±°ì—ê²Œ ìƒí’ˆì„ ì œê³µí•˜ëŠ” ê²ƒì´ ì¢‹ë‹¤. í† í”½ ëª¨ë¸ë§ì„ í•œë‹¤ë©´ ë¸”ë¡œê±°ì˜ ì „ë¬¸ì„± ë° ì „ë¬¸ ì£¼ì œë¥¼ ë¹ ë¥´ê²Œ íŒŒì•…í•  ìˆ˜ ìˆë‹¤. 

<br>
<br>

## ê³¼ì œ ëª©í‘œ
 * ì²« ë²ˆì§¸ëŠ” ë‚´ ë¸”ë¡œê·¸ì˜ ì£¼ì œë¥¼ ì°¾ëŠ” ê²ƒì´ë‹¤. ```KMeans``` ì™€ ```LDA topic modeling``` ì„ ì´ìš©í•´ ì–´ë–¤ ì£¼ì œì˜ í¬ìŠ¤íŒ…ì´ ê°€ì¥ ë§ì€ì§€ ì•Œì•„ë³´ê³  ë¸”ë¡œê·¸ ëŒ€ì£¼ì œë¥¼ ì„ ì •í•œë‹¤.  

 * ë‘ ë²ˆì§¸ëŠ” ì´ë¯¸ í•œ ë¶„ì•¼ì— ëŒ€í•œ ì „ë¬¸ì„±ì„ ì§€ë‹Œ ë¸”ë¡œê·¸ë“¤ì„ ì´ìš©í•´ ê° ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë¶„ì„í•´ ë³´ëŠ” ê²ƒì´ë‹¤.  

 * ì´í›„ ê° ëª¨ë¸ì„ í†µí•œ ê²°ê³¼ë¥¼ ì‹œê°í™”í•œë‹¤.  

 * ë‘ ëª¨ë¸ì˜ ê³¼ì •ê³¼ ê²°ê³¼ë¥¼ ë¹„êµí•˜ì—¬ ë¸”ë¡œê·¸ í† í”½ ëª¨ë¸ë§ì— ìˆì–´ ì–´ë–¤ ê²ƒì´ ë” ì í•©í• ì§€ ì•Œì•„ë³¸ë‹¤.

<br>
<br>

## ìˆ˜í–‰ë°©ë²•
### ë°ì´í„° ìˆ˜ì§‘
 * BeautifulSoup ì´ìš©
 * ë„¤ì´ë²„ ë¸”ë¡œê·¸ (ì•½ 130ëª…, 4000í¬ìŠ¤íŒ…), ë‚´ ë¸”ë¡œê·¸(ì•½ 180 í¬ìŠ¤íŒ…) ì œëª© ë° ë³¸ë¬¸ í¬ë¡¤ë§

### ë°ì´í„° ì „ì²˜ë¦¬
 * Konlpy Okt ì´ìš©í•œ í˜•íƒœì†Œ ë¶„ì„, ëª…ì‚¬ë§Œ ì¶”ì¶œ, í† í°í™”, TF-IDF  
 * ëª¨ë“  ì£¼ì œì—ì„œ ë“±ì¥í•  ìˆ˜ ìˆëŠ” ë‹¨ì–´ ì„ì˜ì ìœ¼ë¡œ ë¶ˆìš©ì–´ ì²˜ë¦¬
 <br>

```python

# tokenize ë¥¼ ìœ„í•œ ë¬¸ì„œí™”
text['guel'] = text.apply(lambda row: (row['guel']), axis=1)

# tokenize
import nltk
text['guel']= text.apply(lambda row: nltk.word_tokenize (row['guel']), axis=1)

# ë¶ˆìš©ì–´ ì œê±°
temp=[]

with open('./stop_words.txt', 'r',encoding='utf-8') as stop:
    lines = stop.readlines()
    for line in lines:
        line=re.sub('\n','',line,0).strip()
        temp.append(line)

stop_words=' '.join(map(str, temp))
stop_words=stop_words.split(' ')

text['stopwords']=text['guel'].apply(lambda x: [word for word in x if word not in stop_words])
tokenized_doc = text['stopwords'].apply(lambda x : [word for word in x if len(word) > 1])

# ì—­í† í°í™”
detokenized_doc = []
for i in range(len(text)):
    t = ' '.join(tokenized_doc[i])
    detokenized_doc.append(t)
text['stopwords'] = detokenized_doc

text.to_csv("plusotherblog_dt.csv", header = True, index=False, encoding='CP949')

# ë¬¸ì„œ ë‚´ ë‹¨ì–´ Count
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
vectorize = CountVectorizer(min_df=2)
X = vectorize.fit_transform(text['stopwords'])
print('(sentence {}, feature {})'.format(X.shape[0], X.shape[1]))

array = pd.DataFrame(X.toarray())
array


```

<br>

### í† í”½ ëª¨ë¸ë§
#### LDA

```Input : ë¬¸ì„œ ë‹¨ì–´ Tf-Idf (ì ìˆ˜ ë†’ì€ 1400ê°œ ë‹¨ì–´)```

![LDAexplain](https://user-images.githubusercontent.com/64299475/86463203-0668c780-bd68-11ea-8e92-998e8ed6bb7d.JPG)

<br>

__ê¹ìŠ¤ìƒ˜í”Œë§__  
<img width="300" alt="4" src="https://user-images.githubusercontent.com/64299475/86463132-e933f900-bd67-11ea-938a-71bab51813e1.png">

__ì˜ˆì‹œ__  
![LDAsampling1](https://user-images.githubusercontent.com/64299475/86463133-eb965300-bd67-11ea-92c5-04daad417f19.JPG)

![LDAsampling2](https://user-images.githubusercontent.com/64299475/86463135-ecc78000-bd67-11ea-9144-db3e6ae455af.JPG)

<br>

```Output: í¬ìŠ¤íŒ… ë³„ ì£¼ì œ```

í¬ìŠ¤íŒ…(ë¬¸ì„œ) ë³„ ì£¼ì œ ë„ì¶œ  
â†’ ë¼ë²¨ë§  
â†’ ë¸”ë¡œê±° ë³„ í¬ìŠ¤íŒ…(ë¬¸ì„œ) ì£¼ì œ ê°œìˆ˜ Count  
â†’ ê°€ì¥ ë§ì´ ë“±ì¥í•œ í¬ìŠ¤íŒ… ì£¼ì œë¥¼ ìµœì¢… ë¸”ë¡œê·¸ ì£¼ì œë¡œ í• ë‹¹  

```Output: ë¸”ë¡œê·¸ ë³„ ì£¼ì œ```

<br>
<br>
<br>

 ```python
# Build Model
lda_model = LatentDirichletAllocation(n_components=22, learning_method='online', random_state=777, max_iter=1)
lda_top = lda_model.fit_transform(Y)
print(lda_model.components_.shape)

# ê° ë‹¨ì–´ ì •ìˆ˜ ì¸ì½”ë”© / ê° ë¬¸ì„œ ë‹¨ì–´ ë¹ˆë„ìˆ˜ ê¸°ë¡
# ë‹¨ì–´ì§‘í•© (1400ê°œ ë‹¨ì–´)
terms = vectorizer.get_feature_names() 
from gensim import corpora
dictionary = corpora.Dictionary(tokenized_doc)
corpus = [dictionary.doc2bow(text) for text in tokenized_doc]

# Model Training
# Topic ë³„ ìµœë¹ˆ ë‹¨ì–´ ì¶”ì¶œ
import gensim

NUM_TOPICS = 22 #22ê°œì˜ í† í”½, k=22
%time ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15, random_state=9)
topics = ldamodel.print_topics(num_words=4)
for topic in topics:
    print(topic)

# ì‹œê°í™”
import pyLDAvis.gensim
pyLDAvis.enable_notebook()
vis = pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary)
pyLDAvis.save_html(vis,'LDA.html')
pyLDAvis.display(vis)

 ```

<br>
<br>
<br>

 #### Kmeans

```Input : L2 Norm ì •ê·œí™”í•œ ë°ì´í„° t-sne ê·¸ë˜í”„ì— ì„ë² ë”©```

![Kmeans](https://user-images.githubusercontent.com/64299475/86463248-17b1d400-bd68-11ea-8b43-c7d399683b62.png)

<br>

```Output: í¬ìŠ¤íŒ… ë³„ ì£¼ì œ```

í¬ìŠ¤íŒ…(ë¬¸ì„œ) ë³„ ì£¼ì œ ë„ì¶œ  
â†’ ë¼ë²¨ë§  
â†’ ë¸”ë¡œê±° ë³„ í¬ìŠ¤íŒ…(ë¬¸ì„œ) ì£¼ì œ ê°œìˆ˜ Count  
â†’ ê°€ì¥ ë§ì´ ë“±ì¥í•œ í¬ìŠ¤íŒ… ì£¼ì œë¥¼ ìµœì¢… ë¸”ë¡œê·¸ ì£¼ì œë¡œ í• ë‹¹  

```Output: ë¸”ë¡œê·¸ ë³„ ì£¼ì œ```

<br>
<br>
<br>

```python
naverData = pd.read_csv('plusotherblog_dt.csv',encoding='CP949')
docs = naverData['stopwords'].values.tolist()

# ì „ì²˜ë¦¬
# Count vector -> L2 ì •ê·œí™” (ë‹¨ì–´ ë¬¸ì„œ ìœ í´ë¦¬ë“œ ê±°ë¦¬)
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(naverData['stopwords'].values.astype('U'))
X = normalize(X, norm='l2')

# 100ê°œì˜ cluster ì¤‘ì‹¬
# Kmeans Training
%time kmeans = KMeans(n_clusters=100,random_state=0).fit(X)
labels = kmeans.labels_
centers = kmeans.cluster_centers_

# ì‹œê°í™” (use pyLDAvis)
vocab = [vocab for vocab, idx in sorted(vectorizer.vocabulary_.items(), key=lambda x:x[1])]

prepared_data = kmeans_to_prepared_data(
    X, vocab, centers, labels,
    embedding_method='tsne')

pyLDAvis.display(prepared_data)

```

<br>
<br>

### í‰ê°€
 * LDA: Topic Coherence (0.62)
 * Kmeans: Average of Word2Vec Similarity (0.43)

```python
from __future__ import print_function
from gensim.models import KeyedVectors
ko_model = KeyedVectors.load_word2vec_format('wiki.ko.vec')

data = pd.DataFrame(prepared_data.token_table)

k=1
all=0
co=0

excepttopic=0 

while(k<=100):
    temp=0
    exceptnum=0
    filter = data[data['Topic'] == k]
    
    # êµ°ì§‘ ë‚´ ë‹¨ì–´ìˆ˜ 5 ë¯¸ë§Œì¸ ë¬¸ì„œ ê³ ë ¤ ì•ˆ í•¨
    if (len(filter) > 5):
        print("-------------------")
    else:
        print("-------------------")
        excepttopic+=1
        pass
        
    word = pd.DataFrame(filter['Term'])
    word= pd.DataFrame(word.reset_index()['Term'])
    
    # êµ°ì§‘ ë‚´ ìƒìœ„ 5ê°œ ë‹¨ì–´ ë¹„êµ
    for i in range(5):
        for j in range(5):
            try:
                term1= (''.join(map(str,word.values[i])))
                term2= (''.join(map(str, word.values[j])))
                sim_score = ko_model.wv.similarity(term1, term2)
            except:
                exceptnum+=1
                pass
            temp+=(sim_score)
            
    temp= temp-5+exceptnum
    co= ((temp)/((5-exceptnum)*(4-exceptnum))) 
    # êµ°ì§‘ ë³„ í‰ê·  word2vec ì ìˆ˜
    #'ë•ì§ˆ'ê³¼ ê°™ì€ vecì— ì—†ëŠ” ë‹¨ì–´ ê³ ë ¤ ì•ˆ í•¨

    # other êµ°ì§‘ ì œê±°
    if k in [13,15,21,22,28,32,45,46,48,54,64,88]:
        excepttopic+=1
        co=0
        print(co)
        
    all+=co
    k+=1

# ëª¨ë“  êµ°ì§‘ í‰ê·  word2vec ì ìˆ˜
all= all/(100-excepttopic)

print(">>> Kmeans Word2Vec similarity average : ", all)
print(">>> í¬í•¨ë˜ì§€ ì•Šì€ Clustering ê°œìˆ˜ : " ,excepttopic)

```

<br>
<br>

## ê³¼ì œ ìˆ˜í–‰ ê²°ê³¼
### ë‚´ ë¸”ë¡œê·¸ ì£¼ì œ ë¹„ìœ¨
 * LDA: ë§›ì§‘(71) / ìœ¡ì•„&ì• ì™„ë™ë¬¼(35) / ë·°í‹°(11) / ìŒì•…(10)
 * Kmeans: ë§›ì§‘(52) / IT(21) / ì—¬í–‰(10) / ìŒì•…(8) / ë·°í‹°(7)  

 * LDAì™€ Kmeans ëª¨ë‘ 1ìˆœìœ„ ì£¼ì œëŠ” ```ë§›ì§‘``` ì´ë¼ëŠ” ë™ì¼í•œ ê²°ê³¼ê°€ ë‚˜ì™”ì§€ë§Œ, 2ìˆœìœ„ë¶€í„°ëŠ” ìƒë‹¹íˆ ë‹¤ë¥¸ ê²°ê³¼ê°€ ë‚˜ì™”ë‹¤. ì‹¤ì œ ë‚˜ì˜ ë¸”ë¡œê·¸ëŠ” Kmeansì˜ ê²°ê³¼ì™€ ë” ë¹„ìŠ·í•˜ë‹¤. 

### ì‹¤ì œ ë¸”ë¡œê·¸ ì£¼ì œì™€ì˜ ë¹„êµ
 * LDAëŠ” ```65.15%``` ì˜ ì •ë‹µë¥ ì„ ë³´ì˜€ë‹¤. êµ°ì§‘ì´ ë§Œë“¤ì–´ì§€ì§€ ì•Šì€ DIY, Book, Animation ì£¼ì œë¥¼ ì œì™¸í•˜ë©´ ```73.50%``` ì˜ ì •ë‹µë¥ ì„ ë³´ì¸ë‹¤.
 * KMeansëŠ” ì²˜ìŒ ê³„íší–ˆë˜ 18ê°œ ì£¼ì œ ëª¨ë‘ êµ°ì§‘ì´ ë§Œë“¤ì–´ì¡Œìœ¼ë©°, ì •ë‹µë¥ ì€ ```74.24%```ë¥¼ ë³´ì¸ë‹¤.

### êµ°ì§‘í™” ì‹œê°„
 * LDA: ì•½ 5ë¶„ ì†Œìš”
 * K-means: ì•½ 15ë¶„ ì†Œìš”  

 * í•˜ì§€ë§Œ LDAëŠ” ë‹¨ì–´ ìˆ˜ë¥¼ __1200ê°œ__ ë¡œ ê³ ì •í•´ë‘” ìƒíƒœì´ë©°, K-meansëŠ” ë³„ë„ì˜ ì„¤ì • ì—†ì´ __30000ê°œ__ ë¥¼ ì‚¬ìš©í•œ ìƒíƒœì´ê¸° ë•Œë¬¸ì— ë‹¨ìˆœ ë¹„êµëŠ” ì–´ë µë‹¤.

<br>

### ì‹œê°í™”ìë£Œ

![LDA](https://user-images.githubusercontent.com/64299475/86433234-1fa45080-bd35-11ea-89c9-b2a3daffc87d.PNG)

![Kmeans](https://user-images.githubusercontent.com/64299475/86433246-2763f500-bd35-11ea-8b56-95af66d74bb8.PNG)
![Kmeans](https://user-images.githubusercontent.com/64299475/86433261-321e8a00-bd35-11ea-8a91-24a30b833b53.PNG)

 <br>
 <br>

## ê²°ë¡ 
### LDA
 * ì¥ì  ë° ì¶”ì²œ ìš©ë„: ë¸”ë¡œê·¸ë‚˜ íŠ¸ìœ„í„° ë¬¸ì„œì²˜ëŸ¼ ```í•œ ë¬¸ì„œë‹¹ ì£¼ì œê°€ ì—¬ëŸ¬ ê°œì¼ ë•Œ``` ì‚¬ìš©í•˜ê¸° ì í•©í•˜ë‹¤. ë˜í•œ, ë¶„ì„ ë¹„ìš©ì´ ë§ì´ ë“¤ê¸° ë•Œë¬¸ì— ```ì†ŒëŸ‰ì˜ ë¬¸ì„œ / ì ì€ ì£¼ì œ``` ë¥¼ êµ°ì§‘í™”í•  ë•Œ ì í•©í•´ ë³´ì¸ë‹¤.  

 * Kmeans ì— ë¹„í•´ other(ê¸°íƒ€) êµ°ì§‘ì„ ì²˜ë¦¬í•˜ê¸°ê°€ ì–´ë µê¸° ë•Œë¬¸ì— ìµœëŒ€í•œ ì£¼ì œê°€ í™•ì‹¤í•˜ê³  ```ê¹”ë”í•œ Data``` ë¥¼ ì‚¬ìš©í•  ë•Œ ì‚¬ìš©í•˜ê¸° ì¢‹ë‹¤. 

### Kmeans
 * ì¥ì  ë° ì¶”ì²œ ìš©ë„: í•œ ë¬¸ì„œë‹¹ í•˜ë‚˜ì˜ ì£¼ì œë§Œ ë‹´ì„ ìˆ˜ ìˆë‹¤ê³  ê°€ì •ëœ ëª¨ë¸ì´ê¸° ë•Œë¬¸ì— ì‹ ë¬¸ê¸°ì‚¬ì²˜ëŸ¼ ```ì£¼ì œê°€ í•˜ë‚˜ì¸ ê²½ìš°``` ì— ì‚¬ìš©í•˜ê¸° ì í•©í•˜ë‹¤. ë˜í•œ, Input data ëŒ€ë¹„ ë¶„ì„ì´ ë¹ ë¥´ê²Œ ì´ë£¨ì–´ì§€ê¸° ë•Œë¬¸ì— ```ë‹¤ëŸ‰ì˜ ë¬¸ì„œ / ë§ì€ ì£¼ì œ``` ë¥¼ êµ°ì§‘í™”í•  ë•Œ ì í•©í•˜ë‹¤ê³  ìƒê°í•œë‹¤.  
 
 * LDAì™€ ë°˜ëŒ€ë¡œ other(ê¸°íƒ€) êµ°ì§‘ì„ ì„ ë³„í•˜ê³  ì²˜ë¦¬í•˜ê¸°ê°€ í¸ë¦¬í•˜ë‹¤.  

<br>
<br>

## schedule
### 3ì›”
 * ì£¼ì œ ì„ ì • (ìˆ˜í•„ ì“°ëŠ” AI â†’ ì£¼ì œ ë³€ê²½) âœ”

### 4ì›”
 * ë°ì´í„° í¬ë¡¤ë§ (íƒ€ ë¸”ë¡œê±° í¬ìŠ¤íŒ… í¬í•¨) âœ” 
 * ë°ì´í„° ì ê²€ ë° ì •ì œ âœ” 
 * ë°ì´í„° í† í°í™” âœ” 
 * ë°ì´í„° ì „ì²˜ë¦¬ âœ”

### 5ì›”
 * Kmeans ëª¨ë¸ë§ (Decision Tree  â†’ ëª¨ë¸ ë³€ê²½) âœ” 
 * LDA ëª¨ë¸ë§ âœ”

### 6ì›”
 * ëª¨ë¸ í‰ê°€ ë° ìˆ˜ì • âœ”
 * ë¸”ë¡œê·¸ ì£¼ì œ ì„ ì • âœ”
 * ëª¨ë¸ ë¹„êµ ë° ë¶„ì„ âœ”
